# Stress-Testing-LM-truthfulness

## Introduction

Large Language Models (LLMs) have rapidly advanced in recent years, powering applications across diverse domains, from conversational agents to information retrieval systems. Despite their impressive capabilities, concerns remain about the factual accuracy and reliability of their outputs. Truthfulness is a critical property for language models, especially when deployed in real-world scenarios where erroneous information can lead to significant consequences.

This thesis project aims to systematically evaluate and "stress-test" the truthfulness of LLMs under various conditions. By designing targeted benchmarks and scenarios, we explore how these models respond to ambiguous, misleading, or adversarial prompts. Our methodology involves both quantitative and qualitative assessments, focusing on factors that influence the models’ likelihood of generating truthful versus misleading responses.

A key component of our research is the comparison between traditional evaluation metrics and the emerging approach of using LLMs themselves as automated judges for assessing truthfulness. We analyze how conventional metrics fare against LLM-as-a-judge frameworks, exploring the strengths and limitations of each method. This comparative analysis is intended to provide deeper insight into the reliability and robustness of LLM truthfulness evaluations.

Ultimately, the goal of this research is to identify strengths and limitations in current LLM architectures regarding truthfulness, and to propose strategies for improving their reliability. The findings will contribute towards building safer, more trustworthy AI systems for widespread use.

## Demo App (under const) 

<img width="1422" height="1020" alt="image" src="https://github.com/user-attachments/assets/0b3676ee-86af-4b1b-8378-7651477d1298" />

